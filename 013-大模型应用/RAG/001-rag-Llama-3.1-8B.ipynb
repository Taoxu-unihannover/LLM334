{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cac2bec",
   "metadata": {},
   "source": [
    "## 基于LangChain和Llama-3.1搭建RAG(检索增强生成)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e158f897",
   "metadata": {},
   "source": [
    "### 环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfa20c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UBUNTU 22.04\n",
    "# CUDA VERSION = 12.2\n",
    "# NVDIA GPU > 24GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0138a7",
   "metadata": {},
   "source": [
    "### 环境配置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6782ace",
   "metadata": {},
   "source": [
    "新建Conda环境：\n",
    "\n",
    "conda create --name rag_langchain python=3.10\n",
    "\n",
    "激活刚刚新建的Conda环境：\n",
    "\n",
    "conda activate rag_langchain\n",
    "\n",
    "接下来配置部署LLM的工具，这里我选择了MS-SWIFT[2]\n",
    "\n",
    "首先下载源码（服务器网络环境不佳的可以先下载的自己的机器然后再上传到服务器并解压）：\n",
    "\n",
    "git clone https://github.com/modelscope/swift.git\n",
    "\n",
    "进入路径并安装：\n",
    "\n",
    "cd ./your_path/swift\n",
    "\n",
    "pip install -e .[llm]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa7b992",
   "metadata": {},
   "source": [
    "拷贝LLM权重，国内推荐从魔塔社区modelscope去下载  \n",
    "\n",
    "modelscope download --model=LLM-Research/Meta-Llama-3.1-8B-Instruct --local_dir Meta-Llama-3.1-8B-Instruct\n",
    "\n",
    "向量数据库和langchain的安装\n",
    "\n",
    "pip install langchain openai weaviate-client\n",
    "\n",
    "至此，项目所需的环境已经配置完毕。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ea15a1",
   "metadata": {},
   "source": [
    "### 下载embedding模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f1cfd4",
   "metadata": {},
   "source": [
    "设置huggingface 国内环境变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cda6b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U huggingface_hub\n",
    "# !export HF_ENDPOINT=https://hf-mirror.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf19af24",
   "metadata": {},
   "source": [
    "下载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88a7733d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! huggingface-cli download --local-dir-use-symlinks False --resume-download shibing624/text2vec-base-chinese --local-dir shibing624/text2vec-base-chinese"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b128e7fa",
   "metadata": {},
   "source": [
    "### 数据准备与存储向量数据库"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b800c3",
   "metadata": {},
   "source": [
    "这里我从网上摘录了一些关于RAG的知识保存在文件langchain_knowledge.txt中，内容如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4d9d2d",
   "metadata": {},
   "source": [
    "RAG Architecture\n",
    "典型的 RAG 应用程序有两个主要组件：\n",
    "\n",
    "索引（Indexing）\n",
    "用于从源获取数据并为其建立索引的管道。这通常发生在离线状态。\n",
    "\n",
    "提取和生成（Retriever and generation）\n",
    "实际的 RAG 链，它在运行时接受用户查询并从索引中检索相关数据，然后将其传递给模型。\n",
    "\n",
    "索引（Indexing）\n",
    "\n",
    "Load\n",
    "首先需要加载数据，通过DocumentLoaders完成\n",
    "\n",
    "Split\n",
    "Text splitters将large Documents分成更小的chunks。这对于索引数据和将其传递到模型都很有用，因为大块更难搜索并且不适合模型的有限上下文窗口。\n",
    "\n",
    "Store\n",
    "存储和索引我们的分割，这通常是使用 VectorStore 和 Embeddings 模型来完成的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b212912",
   "metadata": {},
   "source": [
    "#### 加载文档数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f8bd5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '/home/taoxu/test02/taoxu/101-Rag/data/langchain_knowledge.txt'}, page_content='LangChain\\nLangChain是一个软件开发框架，可以更轻松地使用大型语言模型（LLM）创建应用程序。它是一个具有 Python 和 JavaScript 代码库的开源工具。LangChain 允许开发人员将 GPT-4 等 LLM 与外部数据相结合，为聊天机器人、代码理解、摘要等各种应用程序开辟了可能性。\\n\\nLangChain模块\\nLangChain将其功能分组到以下模块中：\\n\\n模型\\n提示\\n链\\n代理\\n记忆\\n文档加载程序和索引\\n提示\\n        提示是指模型输入。在前面的部分中，您将提示硬编码为 LLM 和聊天模型。此技术不适用，因为在生产环境中不会收到硬编码的完整文本提示。相反，您将收到来自用户的简洁输入，您将希望将其转换为提示。\\n\\n模型\\nLangChain支持三种类型的模型：\\n\\n大型语言模型\\n聊天模型\\n文本嵌入模型\\n链\\n链允许您同时运行多个LangChain模块。例如，使用链，您可以同时运行提示符和 LLM，从而避免了首先格式化 LLM 模型的提示，然后使用模型在单独的步骤中执行它。\\n\\nLangChain支持三种主要类型的链：\\n\\n简单的 LLM 链\\n顺序链\\n定制链\\n代理\\nLangChain代理涉及LLM来执行以下步骤：\\n\\n根据用户输入或其先前的输出确定要执行的操作。\\n执行操作。\\n观察输出。\\n重复前三个步骤，直到它尽其所能完成用户输入中定义的任务。\\nRAG Architecture\\n典型的 RAG 应用程序有两个主要组件：\\n\\n索引（Indexing）\\n用于从源获取数据并为其建立索引的管道。这通常发生在离线状态。\\n\\n提取和生成（Retriever and generation）\\n实际的 RAG 链，它在运行时接受用户查询并从索引中检索相关数据，然后将其传递给模型。\\n\\n索引（Indexing）\\n\\nLoad\\n首先需要加载数据，通过DocumentLoaders完成\\n\\nSplit\\nText splitters将large Documents分成更小的chunks。这对于索引数据和将其传递到模型都很有用，因为大块更难搜索并且不适合模型的有限上下文窗口。\\n\\nStore\\n存储和索引我们的分割，这通常是使用 VectorStore 和 Embeddings 模型来完成的。')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "txt_file_path = \"/home/taoxu/test02/taoxu/101-Rag/data/langchain_knowledge.txt\"\n",
    "loader = TextLoader(txt_file_path)\n",
    "document_raw = loader.load()\n",
    "print(document_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63b4fa3",
   "metadata": {},
   "source": [
    "#### 使用CharacterTextSplitter来分割文本："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0c1fe9",
   "metadata": {},
   "source": [
    "对文档进行切片，避免文档太长超过大模型最大输入长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7cfbe43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '/home/taoxu/test02/taoxu/101-Rag/data/langchain_knowledge.txt'}, page_content='LangChain\\nLangChain是一个软件开发框架，可以更轻松地使用大型语言模型（LLM）创建应用程序。它是一个具有 Python 和 JavaScript 代码库的开源工具。LangChain 允许开发人员将 GPT-4 等 LLM 与外部数据相结合，为聊天机器人、代码理解、摘要等各种应用程序开辟了可能性。\\n\\nLangChain模块\\nLangChain将其功能分组到以下模块中：\\n\\n模型\\n提示\\n链\\n代理\\n记忆\\n文档加载程序和索引\\n提示\\n        提示是指模型输入。在前面的部分中，您将提示硬编码为 LLM 和聊天模型。此技术不适用，因为在生产环境中不会收到硬编码的完整文本提示。相反，您将收到来自用户的简洁输入，您将希望将其转换为提示。\\n\\n模型\\nLangChain支持三种类型的模型：\\n\\n大型语言模型\\n聊天模型\\n文本嵌入模型\\n链\\n链允许您同时运行多个LangChain模块。例如，使用链，您可以同时运行提示符和 LLM，从而避免了首先格式化 LLM 模型的提示，然后使用模型在单独的步骤中执行它。\\n\\nLangChain支持三种主要类型的链：'), Document(metadata={'source': '/home/taoxu/test02/taoxu/101-Rag/data/langchain_knowledge.txt'}, page_content='LangChain支持三种主要类型的链：\\n\\n简单的 LLM 链\\n顺序链\\n定制链\\n代理\\nLangChain代理涉及LLM来执行以下步骤：\\n\\n根据用户输入或其先前的输出确定要执行的操作。\\n执行操作。\\n观察输出。\\n重复前三个步骤，直到它尽其所能完成用户输入中定义的任务。\\nRAG Architecture\\n典型的 RAG 应用程序有两个主要组件：\\n\\n索引（Indexing）\\n用于从源获取数据并为其建立索引的管道。这通常发生在离线状态。\\n\\n提取和生成（Retriever and generation）\\n实际的 RAG 链，它在运行时接受用户查询并从索引中检索相关数据，然后将其传递给模型。\\n\\n索引（Indexing）\\n\\nLoad\\n首先需要加载数据，通过DocumentLoaders完成\\n\\nSplit\\nText splitters将large Documents分成更小的chunks。这对于索引数据和将其传递到模型都很有用，因为大块更难搜索并且不适合模型的有限上下文窗口。\\n\\nStore\\n存储和索引我们的分割，这通常是使用 VectorStore 和 Embeddings 模型来完成的。')]\n"
     ]
    }
   ],
   "source": [
    "# 使用CharacterTextSplitter来分割文本，设置chunk_size大约为500，chunk_overlap为50\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "documents = text_splitter.split_documents(document_raw)\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ec980c",
   "metadata": {},
   "source": [
    "#### 自己写一个文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9afec46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"狗是很好的伴侣，以忠诚和友好著称。\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"猫是独立的宠物，猫常常喜欢自己的空间。\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"金鱼是初学者的热门宠物，护理相对简单。\",\n",
    "        metadata={\"source\": \"fish-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"鹦鹉是聪明的鸟类，能够模仿人类语言。\",\n",
    "        metadata={\"source\": \"bird-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"兔子是社会性动物，需要足够的空间跳跃。\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549004d3",
   "metadata": {},
   "source": [
    "#### 切片后的文本进行向量化，保存到向量数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46f40591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceBgeEmbeddings\n",
    "BGE_MODEL_PATH = \"/home/taoxu/test02/taoxu/cache/shibing624/text2vec-base-chinese\"\n",
    "huggingface_bge_embedding = HuggingFaceBgeEmbeddings(model_name=BGE_MODEL_PATH)\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents,\n",
    "    embedding=huggingface_bge_embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8983eda2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'mammal-pets-doc'}, page_content='狗是很好的伴侣，以忠诚和友好著称。'),\n",
       " Document(metadata={'source': 'fish-pets-doc'}, page_content='金鱼是初学者的热门宠物，护理相对简单。'),\n",
       " Document(metadata={'source': 'mammal-pets-doc'}, page_content='兔子是社会性动物，需要足够的空间跳跃。'),\n",
       " Document(metadata={'source': 'mammal-pets-doc'}, page_content='猫是独立的宠物，猫常常喜欢自己的空间。')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search(\"狗\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24120ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'mammal-pets-doc'}, page_content='狗是很好的伴侣，以忠诚和友好著称。'),\n",
       " Document(metadata={'source': 'fish-pets-doc'}, page_content='金鱼是初学者的热门宠物，护理相对简单。'),\n",
       " Document(metadata={'source': 'mammal-pets-doc'}, page_content='兔子是社会性动物，需要足够的空间跳跃。'),\n",
       " Document(metadata={'source': 'mammal-pets-doc'}, page_content='猫是独立的宠物，猫常常喜欢自己的空间。')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await vectorstore.asimilarity_search(\"狗\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6278e087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'source': 'mammal-pets-doc'}, page_content='狗是很好的伴侣，以忠诚和友好著称。'),\n",
       "  350.973388671875),\n",
       " (Document(metadata={'source': 'fish-pets-doc'}, page_content='金鱼是初学者的热门宠物，护理相对简单。'),\n",
       "  400.5721130371094),\n",
       " (Document(metadata={'source': 'mammal-pets-doc'}, page_content='兔子是社会性动物，需要足够的空间跳跃。'),\n",
       "  405.5841064453125),\n",
       " (Document(metadata={'source': 'mammal-pets-doc'}, page_content='猫是独立的宠物，猫常常喜欢自己的空间。'),\n",
       "  424.0150146484375)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search_with_score(\"狗\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d37ef466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'mammal-pets-doc'}, page_content='狗是很好的伴侣，以忠诚和友好著称。')],\n",
       " [Document(metadata={'source': 'fish-pets-doc'}, page_content='金鱼是初学者的热门宠物，护理相对简单。')]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 1},\n",
    ")\n",
    "\n",
    "retriever.batch([\"狗\", \"金鱼\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1080787",
   "metadata": {},
   "source": [
    "### 大模型部署"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7980fb5",
   "metadata": {},
   "source": [
    "在终端运行swift部署模型的命令："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f685854",
   "metadata": {},
   "source": [
    "CUDA_VISIBLE_DEVICES=0 swift deploy --model_id_or_path /home/taoxu/test02/taoxu/cache/Meta-Llama-3.1-8B-Instruct --model_type llama3_1-8b-instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c65ffbe",
   "metadata": {},
   "source": [
    "其中CUDA_VISIBLE_DEVICES是用来指定模型加载到哪张卡上，单卡可以忽略不写\n",
    "\n",
    "部署之后，可以用下面的代码来测试是否部署成功"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3bb5c5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_type: llama3_1-8b-instruct\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    api_key='EMPTY', #随便填\n",
    "    base_url='http://localhost:8000/v1', #填部署成功后的地址+端口+v1\n",
    ")\n",
    "model_type = client.models.list().data[0].id\n",
    "print(f'model_type: {model_type}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74819cc8",
   "metadata": {},
   "source": [
    "若模型正确部署，则会输出之前在终端指定的model_type值，例如这里是：model_type: llama3_1-8b-instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348d4055",
   "metadata": {},
   "source": [
    "### 用openai的格式访问之前部署的大模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e448f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "API_SECRET_KEY = \"EMPTY\"\n",
    "BASE_URL = \"http://localhost:8000/v1\"\n",
    "\n",
    "# 设置环境变量\n",
    "os.environ[\"OPENAI_API_KEY\"] = API_SECRET_KEY\n",
    "os.environ[\"OPENAI_API_BASE\"] = BASE_URL\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"llama3_1-8b-instruct\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ffc6983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='金鱼是一种常见的宠物鱼，人们喜欢养金鱼的原因有很多。首先，金鱼的颜色多样，种类繁多，人们可以根据自己的喜好选择喜欢的颜色和形状的金鱼。其次，金鱼的生长速度快，养起来比较容易，不需要太多的空间和照顾。再次，金鱼的价格便宜，人们可以在不花太多钱的情况下拥有自己的金鱼。最后，金鱼的观赏性强，人们可以在观赏金鱼的过程中放松身心，减压。', response_metadata={'token_usage': {'completion_tokens': 135, 'prompt_tokens': 15, 'total_tokens': 150}, 'model_name': 'llama3_1-8b-instruct', 'system_fingerprint': None, 'finish_reason': None, 'logprobs': None}, id='run-3edc1ed3-89fa-42f5-855b-38a40e3e0a90-0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"请聊聊金鱼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "34d10062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "message = \"\"\"\n",
    "请根据我提供的内容回答问题.\n",
    "\n",
    "{question}\n",
    "\n",
    "提供的内容:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"human\", message)])\n",
    "\n",
    "rag_chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e7364fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "金鱼是初学者的热门宠物，护理相对简单。\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"请聊聊金鱼\")\n",
    "\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
